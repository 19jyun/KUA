{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AKI9RmLGpK-"
      },
      "source": [
        "# COSE461 Assignment 5\n",
        "### **Due 11:59 PM, Fri May. 17 (Extended)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Pretrained Transformer models and knowledge access\n",
        "You’ll train a Transformer to perform a task that involves accessing knowledge about the world – knowledge which isn’t provided via the task’s training data (at least if you want to generalize outside the training set). You’ll find that it more or less fails entirely at the task. You’ll then learn how to pretrain that Transformer on Wikipedia text that contains world knowledge, and find that finetuning that Transformer on the same knowledge-intensive task enables the model to access some of the knowledge learned at pretraining time. You’ll find that this enables models to perform considerably above chance on a held out development set.\n",
        "\n",
        "The code you’re provided with is a fork of Andrej Karpathy’s **minGPT**. It’s nicer than most research code in that it’s relatively simple and transparent. The “GPT” in minGPT refers to the Transformer language model of OpenAI.\n"
      ],
      "metadata": {
        "id": "0T8PFVf-Mct5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm COSE461_Assignment5 -rf\n",
        "!git clone https://github.com/ku-dmlab/COSE461_Assignment5.git\n",
        "import os\n",
        "os.chdir(\"/content/COSE461_Assignment5\")"
      ],
      "metadata": {
        "id": "6D5WqfhJfzRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f641db39-2d00-41e9-cfa9-b9d557146680"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COSE461_Assignment5'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 21 (delta 3), reused 21 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (21/21), 206.25 KiB | 2.40 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1MQ3FZMhO2N"
      },
      "source": [
        "## Part 1.1 minGPT\n",
        "\n",
        "**Check out the code below.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE1. minGPT model**"
      ],
      "metadata": {
        "id": "VqojGxnXnGVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GPT model:\n",
        "- the initial stem consists of a combination of token encoding and a positional encoding\n",
        "- the meat of it is a uniform sequence of Transformer blocks\n",
        "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "    - all blocks feed into a central residual pathway similar to resnets\n",
        "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
        "    n_layer = 12\n",
        "    n_head = 12\n",
        "    n_embd = 768\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "5Y4hNNktm2zT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE2. minGPT trainer**"
      ],
      "metadata": {
        "id": "ieIpFRQ4nTrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
        "so nothing in this file really has anything to do with GPT specifically.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import logging\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_dataset, test_dataset, config):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = config\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
        "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = raw_model.configure_optimizers(config)\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset if is_train else self.test_dataset\n",
        "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                                batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers)\n",
        "\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            for it, (x, y) in pbar:\n",
        "\n",
        "                # place data on the correct device\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    logits, loss = model(x, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                if is_train:\n",
        "\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    # report progress\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "\n",
        "            if not is_train:\n",
        "                test_loss = float(np.mean(losses))\n",
        "                logger.info(\"test loss: %f\", test_loss)\n",
        "                return test_loss\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "        for epoch in range(config.max_epochs):\n",
        "\n",
        "            run_epoch('train')\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch('test')\n",
        "\n",
        "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
        "            good_model = self.test_dataset is None or test_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = test_loss\n",
        "                self.save_checkpoint()\n"
      ],
      "metadata": {
        "id": "HH52ZE4-nXu9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE3. minGPT Utils**"
      ],
      "metadata": {
        "id": "WLSDDit4nqzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def evaluate_places(filepath, predicted_places):\n",
        "    \"\"\" Computes percent of correctly predicted birth places.\n",
        "    Arguments:\n",
        "      filepath: path to a file with our name, birth place data.\n",
        "      predicted_places: a list of strings representing the\n",
        "          predicted birth place of each person.\n",
        "    Returns: (total, correct), floats\n",
        "    \"\"\"\n",
        "    with open(filepath) as fin:\n",
        "        lines = [x.strip().split('\\t') for x in fin]\n",
        "        if len(lines[0]) == 1:\n",
        "            print('No gold birth places provided; returning (0,0)')\n",
        "            return (0,0)\n",
        "        true_places = [x[1] for x in lines]\n",
        "        total = len(true_places)\n",
        "        assert total == len(predicted_places)\n",
        "        correct = len(list(filter(lambda x: x[0] == x[1],\n",
        "                                  zip(true_places, predicted_places))))\n",
        "        return (float(total),float(correct))"
      ],
      "metadata": {
        "id": "YQ72GxN5nrHz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08XMZPM8TDLn"
      },
      "source": [
        "## Part 1.2 Datasets\n",
        "**Read   through**   NameDataset**,   our   dataset   for   reading   name-birthplace   pairs.**\n",
        "\n",
        "The  task  we’ll  be  working  on  with  our  pretrained  models  is  attempting  to  access  the  birth  place  of a   notable   person,   as   written   in   their   Wikipedia   page.    We’ll   think   of   this   as   a   particularly   simple form  of  question  answering:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Q: \\, Where \\;was\\;[person]\\;born?$\n",
        "\n",
        "$A: \\, [place]$"
      ],
      "metadata": {
        "id": "SbcbiWKkO2Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, you’ll find the the class NameDataset, which reads a TSV (tab-separated values) file of name/place pairs and produces examples of the above form that we can feed to our Transformer model. To make sure that two datasets (dataset for finetuning and the dataset for pretraining) share the same vocabulary (word-integer mapping), NameDataset gets the pretraining dataset as an input.\n"
      ],
      "metadata": {
        "id": "BuCPV67fPqN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "**Vocabulary Specification**\n",
        "\n",
        "The vocabulary is accessible via two dictionaries:\n",
        "\n",
        "  `self.stoi`: a dictionary from characters in the vocabulary to indices of type\n",
        "      int\n",
        "\n",
        "  `self.itos`: a dictionary from indices of type int to characters in the\n",
        "      vocabulary\n",
        "      \n",
        "Identifier `0` is assigned to the unicode element `u\"\\u25A1\"`. This is the empty_square_character. Further, `self.PAD_CHAR = u\"\\u25A1\"`.\n",
        "\n",
        "Identifier `1` is assigned to the unicode element `u\"\\u2047\"`. This is the doublequestionmark character, which we'll use as a sentinel to represent that text is missing from the input. Further, `self.MASK_CHAR = u\"\\u2047\"`.\n",
        "\n",
        "Identifiers `2, ..., len(self.itos)-1` is the sorted list of characters\n",
        "      that appear in the data argument.\n",
        "\n",
        "--------------"
      ],
      "metadata": {
        "id": "4AiQgq5KTNhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\"\"\"\n",
        "The input-output pairs (x, y) of the NameDataset are of the following form:\n",
        "\n",
        "  x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
        "  y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
        "  x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
        "  y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
        "\n",
        "Using the PAD_CHAR characters in y before the ⁇[place] keeps the trainer from\n",
        "optimizing the model to predict the question, \"Where was...\".\n",
        "\n",
        "Note that the NameDataset should take the pretraining_dataset defined in run.py\n",
        "as an input. This is to allow the vocab specification of the NameDataset to be\n",
        "the same as that of the pretraining dataset.\n",
        "\"\"\"\n",
        "\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, pretraining_dataset, data):\n",
        "        self.MASK_CHAR = u\"\\u2047\" # the doublequestionmark character, for mask\n",
        "        self.PAD_CHAR = u\"\\u25A1\" # the empty square character, for pad\n",
        "        self.itos = pretraining_dataset.itos\n",
        "        self.stoi = pretraining_dataset.stoi\n",
        "        self.block_size = pretraining_dataset.block_size\n",
        "        self.data = list(data.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
        "\n",
        "    def __len__(self):\n",
        "        # returns the length of the dataset\n",
        "        return len(self.data) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, oup = self.data[idx].split('\\t')\n",
        "        x = inp + self.MASK_CHAR + oup + self.MASK_CHAR\n",
        "        x = x + self.PAD_CHAR*(self.block_size - len(x))\n",
        "        y = self.PAD_CHAR*(len(inp)-1) + x[len(inp):]\n",
        "\n",
        "        x = x[:-1]\n",
        "        x = torch.tensor([self.stoi[c] for c in x], dtype=torch.long)\n",
        "        y = torch.tensor([self.stoi[c] for c in y], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "1GcGw9Z33ea6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, in the below, we have a dataset for pretraining, to train the model with span corruption objective."
      ],
      "metadata": {
        "id": "n8ZbAFpXR5IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "**Masking Specification**\n",
        "\n",
        "The `__getitem__` function takes an index and returns a data point $(x, y)$ where $x$ and $y$ are Long tensors of length `self.block_size`. $x$ encodes the input sequence, and $y$ encodes the output sequence.\n",
        "\n",
        "0. Use the idx argument of `__getitem__` to retrieve the element of `self.data`\n",
        "at the given index. We'll call the resulting data entry a document.\n",
        "\n",
        "1. Truncate the document to a length no less than `4` characters,\n",
        "and no more than `int(self.block_size*7/8)` characters, where the length is picked *randomly*.\n",
        "\n",
        "2. Now, break the (truncated) document into three substrings:\n",
        "    \n",
        "    `[prefix] [masked_content] [suffix]`\n",
        "\n",
        "  In other words, choose three strings `prefix`, `masked_content` and `suffix`\n",
        "  such that `prefix + masked_content + suffix = [the original document]`.\n",
        "  The length of `[masked_content]` is random, and `1/4` the length of the truncated document on average.\n",
        "\n",
        "3. Rearrange these substrings into the following form:\n",
        "    `[prefix] MASK_CHAR [suffix] MASK_CHAR [masked_content] [pads]`\n",
        "  \n",
        "  This resulting string, denoted `masked_string`, serves as the output example.\n",
        "  Here `MASK_CHAR` is the masking character defined in Vocabulary Specification,\n",
        "  and `[pads]` is a string of repeated `PAD_CHAR` characters chosen so that the\n",
        "  entire string is of length `self.block_size`.\n",
        "\n",
        "  Intuitively, the `[masked_content]`, a string, is removed from the document and replaced with `MASK_CHAR` (the masking character defined in Vocabulary\n",
        "  Specification). After the suffix of the string, the `MASK_CHAR` is seen again,\n",
        "  followed by the content that was removed, and the padding characters.\n",
        "\n",
        "4. We now use `masked_string` to construct the input and output example pair. To\n",
        "do so, simply take the input string to be `masked_string[:-1]`, and the output\n",
        "string to be `masked_string[1:]`. In other words, for each character, the goal is to predict the next character in the masked string.\n",
        "\n",
        "5. Making use of the vocabulary defined, encode the resulting input\n",
        "and output strings as Long tensors and return the resulting data point.\n",
        "\n",
        "--------------"
      ],
      "metadata": {
        "id": "2BhSJPb6Ujcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharCorruptionDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.MASK_CHAR = u\"\\u2047\"  # the doublequestionmark character, for mask\n",
        "        self.PAD_CHAR = u\"\\u25A1\"  # the empty square character, for pad\n",
        "\n",
        "        chars = list(sorted(list(set(data))))\n",
        "        assert self.MASK_CHAR not in chars\n",
        "        assert self.PAD_CHAR not in chars\n",
        "        chars.insert(0, self.MASK_CHAR)\n",
        "        chars.insert(0, self.PAD_CHAR)\n",
        "\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data.split('\\n')\n",
        "        if len(self.data[-1]) == 0:\n",
        "            self.data = self.data[:-1]\n",
        "\n",
        "    def __len__(self):\n",
        "        # returns the length of the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        document = self.data[idx]\n",
        "\n",
        "        # 1. randomly truncate to [4, 7/8 * block_size]\n",
        "        doc_len = len(document)\n",
        "        truncate_len = random.randint(4, int(self.block_size * 7 / 8))\n",
        "        truncate_len = min(doc_len, truncate_len)\n",
        "        truncated_doc = document[:truncate_len]\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        # 2. break to [prefix] [masked_content] [suffix]\n",
        "        # You should assign values to prefix_len and masked_len.\n",
        "\n",
        "        prefix_len = random.randint(1, truncate_len // 2)\n",
        "        masked_len = min(random.randint(1, truncate_len - prefix_len), truncate_len - prefix_len - 1)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        prefix = truncated_doc[:prefix_len]\n",
        "        masked_content = truncated_doc[prefix_len:prefix_len + masked_len]\n",
        "        suffix = truncated_doc[prefix_len + masked_len:]\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        # 3. rearrange to masked_string: [prefix] MASK_CHAR [suffix] MASK_CHAR [masked_content] [pads]\n",
        "        # You should concat strings into masked_string.\n",
        "\n",
        "        total_len = self.block_size - 1  # reserve space for the final MASK_CHAR\n",
        "        masked_string = prefix + self.MASK_CHAR + suffix + self.MASK_CHAR + masked_content + self.PAD_CHAR * (total_len - len(prefix + self.MASK_CHAR + suffix + self.MASK_CHAR + masked_content))\n",
        "\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        # 4. input = masked_string[:-1], output = masked_string[1:]\n",
        "        x = masked_string[:-1]\n",
        "        y = masked_string[1:]\n",
        "\n",
        "        # 5. encode to Long tensors\n",
        "        x = torch.LongTensor([self.stoi[c] for c in x])\n",
        "        y = torch.LongTensor([self.stoi[c] for c in y])\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "1dCVvkDdRp47"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To  get  a  sense  of  the  examples  we’ll  be  working  with,  if  you  run  the  following  code,  it’ll  load  your NameDataset  on  the  training  set  `birth_places_train.tsv`  and  print  out  a  few  examples."
      ],
      "metadata": {
        "id": "OuwUFUv0G36a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corruption_dataset = CharCorruptionDataset(open('wiki.txt', encoding='utf-8').read(), 128)\n",
        "# Make the name dataset\n",
        "name_dataset = NameDataset(corruption_dataset, open('birth_places_train.tsv', encoding='utf-8').read())\n",
        "\n",
        "for _, example in zip(range(4), name_dataset):\n",
        "    x, y = example\n",
        "    print('x:', ''.join([name_dataset.itos[int(c)] for c in x]))\n",
        "    print('y:', ''.join([name_dataset.itos[int(c)] for c in y]))"
      ],
      "metadata": {
        "id": "rzijpvlNGTxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b16cb8c-6b54-406f-c12f-96cb8eb8f26b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n",
            "x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: Where was John Stephen born?⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: Where was Georgina Willis born?⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code loads CharCorruptionDataset on the training set `wiki.txt` and print out a few examples."
      ],
      "metadata": {
        "id": "qAMRZo_bXTvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corruption_dataset = CharCorruptionDataset(open('wiki.txt', encoding='utf-8').read(), 128)\n",
        "for _, example in zip(range(4), corruption_dataset):\n",
        "    x, y = example\n",
        "    print('x:', ''.join([name_dataset.itos[int(c)] for c in x]))\n",
        "    print('y:', ''.join([name_dataset.itos[int(c)] for c in y]))"
      ],
      "metadata": {
        "id": "a9NE7qT_XM2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f7dbf5-4c85-4ac4-fb5a-c2bfd09b222f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n",
            "x: Khatchig Mour⁇h⁇adian. Khatc□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: hatchig Mour⁇h⁇adian. Khatc□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: J⁇o⁇acob Henry Studer. Jacob Henry Studer (26 February 1840 C□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: ⁇o⁇acob Henry Studer. Jacob Henry Studer (26 February 1840 C□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: John Ste⁇rn⁇phen. Bo□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: ohn Ste⁇rn⁇phen. Bo□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "x: Ge⁇ Willis. Geor⁇orgina□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
            "y: e⁇ Willis. Geor⁇orgina□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-szW9avTVbX"
      },
      "source": [
        "## Part 1.3 Fine-Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make predictions (without pretraining).**\n",
        "\n",
        "The below is the code to *fine_tune* and *evaluate* a model."
      ],
      "metadata": {
        "id": "nkQEqdTOQI96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune(writing_params_path, reading_params_path=None, finetune_corpus_path='birth_places_train.tsv', pretrain_corpus_path=\"wiki.txt\"):\n",
        "    # Save the device\n",
        "    device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Keep the block size 128\n",
        "    # Why is the pretraining corpus always required (even if we're not pretraining?)\n",
        "    # It's because we're using it as a hack to always have the same vocabulary\n",
        "    # (that is, the same mapping from character to integer, and we build the\n",
        "    # vocab from the pretraining corpus.)\n",
        "    block_size = 128\n",
        "    text = open(pretrain_corpus_path).read()\n",
        "    pretrain_dataset = CharCorruptionDataset(text, block_size)\n",
        "\n",
        "    # We don't suggest you change these hyperparameters, as they're known to work.\n",
        "    mconf = GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
        "        n_layer=4, n_head=8, n_embd=256)\n",
        "    model = GPT(mconf)\n",
        "\n",
        "    if reading_params_path is not None:\n",
        "        model.load_state_dict(torch.load(reading_params_path))\n",
        "    tconf = TrainerConfig(max_epochs=75,\n",
        "                          batch_size=256,\n",
        "                          learning_rate=6e-4,\n",
        "                          lr_decay=True,\n",
        "                          warmup_tokens=512 * 20,\n",
        "                          final_tokens=200 * len(pretrain_dataset) * block_size,\n",
        "                          num_workers=4)\n",
        "    text = open(finetune_corpus_path, 'r').read()\n",
        "    train_dataset = NameDataset(pretrain_dataset, text)\n",
        "    trainer = Trainer(model, train_dataset, None, tconf)\n",
        "    trainer.train()\n",
        "    # save to writing_params_path\n",
        "    torch.save(model.state_dict(), writing_params_path)\n"
      ],
      "metadata": {
        "id": "yXffapvoQl3J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the `evaluation` code, which samples predictions from the trained model and calls `evaluate_places()` to get the total percentage of correct place\n",
        "predictions."
      ],
      "metadata": {
        "id": "JMAD7le4YgD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(outputs_path, reading_params_path, eval_corpus_path, pretrain_corpus_path=\"wiki.txt\"):\n",
        "    device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "    block_size = 128\n",
        "    text = open(pretrain_corpus_path).read()\n",
        "    pretrain_dataset = CharCorruptionDataset(text, block_size)\n",
        "    mconf = GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
        "        n_layer=4, n_head=8, n_embd=256)\n",
        "    model = GPT(mconf).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(reading_params_path))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with open(outputs_path, 'w') as fout:\n",
        "        predictions = []\n",
        "        for line in tqdm(open(eval_corpus_path)):\n",
        "            x = line.split('\\t')[0]\n",
        "            x = x + '⁇'\n",
        "            x = torch.tensor([pretrain_dataset.stoi[s] for s in x], dtype=torch.long)[None,...].to(device)\n",
        "            pred = sample(model, x, 32, sample=False)[0]\n",
        "            completion = ''.join([pretrain_dataset.itos[int(i)] for i in pred])\n",
        "            pred = completion.split('⁇')[1]\n",
        "            predictions.append(pred)\n",
        "            fout.write(pred + '\\n')\n",
        "        total, correct = evaluate_places(eval_corpus_path, predictions)\n",
        "    if total > 0:\n",
        "        print('Correct: {} out of {}: {}%'.format(correct, total, correct/total*100))\n",
        "    else:\n",
        "        print('Predictions written to {}; no targets provided'\n",
        "                .format(outputs_path))"
      ],
      "metadata": {
        "id": "qNHpFzs5Qyg5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can fine-tune the model by running the below: it shouldn't take more than 10 minutes (given that you are using GPU)."
      ],
      "metadata": {
        "id": "3ZABELyFYrEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train on the names dataset\n",
        "finetune('model.params')"
      ],
      "metadata": {
        "id": "vm7al8VHYqdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4add05-0fd2-4d5e-c3bf-303de965bb0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 7: train loss 0.99342. lr 5.999844e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 2 iter 7: train loss 0.55793. lr 5.999351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 3 iter 7: train loss 0.42747. lr 5.998520e-04: 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\n",
            "epoch 4 iter 7: train loss 0.29548. lr 5.997351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 5 iter 7: train loss 0.26525. lr 5.995844e-04: 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]\n",
            "epoch 6 iter 7: train loss 0.23336. lr 5.993999e-04: 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]\n",
            "epoch 7 iter 7: train loss 0.22995. lr 5.991818e-04: 100%|██████████| 8/8 [00:03<00:00,  2.63it/s]\n",
            "epoch 8 iter 7: train loss 0.21827. lr 5.989299e-04: 100%|██████████| 8/8 [00:02<00:00,  2.85it/s]\n",
            "epoch 9 iter 7: train loss 0.19803. lr 5.986444e-04: 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "epoch 10 iter 7: train loss 0.18290. lr 5.983252e-04: 100%|██████████| 8/8 [00:02<00:00,  2.92it/s]\n",
            "epoch 11 iter 7: train loss 0.17842. lr 5.979723e-04: 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]\n",
            "epoch 12 iter 7: train loss 0.17693. lr 5.975860e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 13 iter 7: train loss 0.16615. lr 5.971660e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 14 iter 7: train loss 0.16825. lr 5.967127e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 15 iter 7: train loss 0.16273. lr 5.962258e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 16 iter 7: train loss 0.16083. lr 5.957056e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 17 iter 7: train loss 0.16348. lr 5.951521e-04: 100%|██████████| 8/8 [00:02<00:00,  3.24it/s]\n",
            "epoch 18 iter 7: train loss 0.16114. lr 5.945654e-04: 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]\n",
            "epoch 19 iter 7: train loss 0.15993. lr 5.939454e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 20 iter 7: train loss 0.15512. lr 5.932923e-04: 100%|██████████| 8/8 [00:02<00:00,  3.15it/s]\n",
            "epoch 21 iter 7: train loss 0.15177. lr 5.926062e-04: 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]\n",
            "epoch 22 iter 7: train loss 0.14561. lr 5.918871e-04: 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]\n",
            "epoch 23 iter 7: train loss 0.14760. lr 5.911352e-04: 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]\n",
            "epoch 24 iter 7: train loss 0.14026. lr 5.903504e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 25 iter 7: train loss 0.14651. lr 5.895329e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 26 iter 7: train loss 0.14502. lr 5.886828e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 27 iter 7: train loss 0.13760. lr 5.878002e-04: 100%|██████████| 8/8 [00:02<00:00,  3.21it/s]\n",
            "epoch 28 iter 7: train loss 0.13633. lr 5.868851e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 29 iter 7: train loss 0.13997. lr 5.859378e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 30 iter 7: train loss 0.13199. lr 5.849582e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 31 iter 7: train loss 0.13319. lr 5.839465e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 32 iter 7: train loss 0.13373. lr 5.829028e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 33 iter 7: train loss 0.12543. lr 5.818272e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 34 iter 7: train loss 0.12559. lr 5.807199e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 35 iter 7: train loss 0.12377. lr 5.795810e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 36 iter 7: train loss 0.11811. lr 5.784106e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 37 iter 7: train loss 0.11816. lr 5.772087e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 38 iter 7: train loss 0.11526. lr 5.759757e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 39 iter 7: train loss 0.11224. lr 5.747116e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 40 iter 7: train loss 0.11016. lr 5.734165e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 41 iter 7: train loss 0.10696. lr 5.720906e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 42 iter 7: train loss 0.10489. lr 5.707341e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "epoch 43 iter 7: train loss 0.10406. lr 5.693470e-04: 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]\n",
            "epoch 44 iter 7: train loss 0.09887. lr 5.679296e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 45 iter 7: train loss 0.09632. lr 5.664820e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 46 iter 7: train loss 0.09417. lr 5.650044e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 47 iter 7: train loss 0.09157. lr 5.634970e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 48 iter 7: train loss 0.08652. lr 5.619598e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 49 iter 7: train loss 0.09291. lr 5.603932e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 50 iter 7: train loss 0.08659. lr 5.587972e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 51 iter 7: train loss 0.08086. lr 5.571720e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "epoch 52 iter 7: train loss 0.07721. lr 5.555179e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 53 iter 7: train loss 0.07022. lr 5.538350e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 54 iter 7: train loss 0.08027. lr 5.521235e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 55 iter 7: train loss 0.07587. lr 5.503835e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 56 iter 7: train loss 0.07123. lr 5.486154e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 57 iter 7: train loss 0.07317. lr 5.468193e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 58 iter 7: train loss 0.07266. lr 5.449953e-04: 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]\n",
            "epoch 59 iter 7: train loss 0.06500. lr 5.431438e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 60 iter 7: train loss 0.06570. lr 5.412648e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 61 iter 7: train loss 0.06463. lr 5.393587e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 62 iter 7: train loss 0.06221. lr 5.374256e-04: 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\n",
            "epoch 63 iter 7: train loss 0.06617. lr 5.354657e-04: 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]\n",
            "epoch 64 iter 7: train loss 0.06021. lr 5.334794e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 65 iter 7: train loss 0.06441. lr 5.314667e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 66 iter 7: train loss 0.05795. lr 5.294279e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 67 iter 7: train loss 0.05955. lr 5.273633e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 68 iter 7: train loss 0.05726. lr 5.252731e-04: 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]\n",
            "epoch 69 iter 7: train loss 0.05782. lr 5.231575e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "epoch 70 iter 7: train loss 0.05445. lr 5.210167e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 71 iter 7: train loss 0.05614. lr 5.188511e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 72 iter 7: train loss 0.05193. lr 5.166608e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 73 iter 7: train loss 0.05453. lr 5.144461e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 74 iter 7: train loss 0.05694. lr 5.122072e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 75 iter 7: train loss 0.05202. lr 5.099444e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the dev set, writing out predictions\n",
        "evaluate(outputs_path=\"nopretrain.dev.predictions\", reading_params_path=\"model.params\", eval_corpus_path=\"birth_dev.tsv\")"
      ],
      "metadata": {
        "id": "M9M6yCMkd6rv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8cc6c7-084a-4a52-90e8-5d13b1d93a77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [00:55,  9.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 3.0 out of 500.0: 0.6%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are the predictions so bad?"
      ],
      "metadata": {
        "id": "1mVSKUBthr8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.4 Pretraining\n",
        "**Make predictions (with pretraining).**\n",
        "\n",
        "The below is the code to *pretrain* a model."
      ],
      "metadata": {
        "id": "8FlMJpnyh8aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain(writing_params_path, pretrain_corpus_path=\"wiki.txt\"):\n",
        "    # Save the device\n",
        "    device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Keep the block size 128\n",
        "    # Why is the pretraining corpus always required (even if we're not pretraining?)\n",
        "    # It's because we're using it as a hack to always have the same vocabulary\n",
        "    # (that is, the same mapping from character to integer, and we build the\n",
        "    # vocab from the pretraining corpus.)\n",
        "    block_size = 128\n",
        "    text = open(pretrain_corpus_path).read()\n",
        "    pretrain_dataset = CharCorruptionDataset(text, block_size)\n",
        "\n",
        "    # We don't suggest you change these hyperparameters, as they're known to work.\n",
        "    mconf = GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
        "        n_layer=4, n_head=8, n_embd=256)\n",
        "    model = GPT(mconf)\n",
        "\n",
        "    tconf = TrainerConfig(max_epochs=650,\n",
        "                          batch_size=128,\n",
        "                          learning_rate=6e-3,\n",
        "                          lr_decay=True,\n",
        "                          warmup_token=512 * 20,\n",
        "                          final_tokens=200 * len(pretrain_dataset) * block_size,\n",
        "                          num_workers=4)\n",
        "    trainer = Trainer(model, pretrain_dataset, None, tconf)\n",
        "    trainer.train()\n",
        "    torch.save(model.state_dict(), writing_params_path)"
      ],
      "metadata": {
        "id": "u9-g0gsFiRaA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrain your model on `wiki.txt` (which should take approximately two hours), finetune it on `NameDataset` and evaluate it."
      ],
      "metadata": {
        "id": "kht5pKB2Yoq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain the model\n",
        "pretrain(\"pretrain.params\")"
      ],
      "metadata": {
        "id": "sMdpM6dvDJbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac746122-628b-4697-a36a-db763dd076f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "epoch 1 iter 22: train loss 3.36057. lr 5.920992e-06: 100%|██████████| 23/23 [00:04<00:00,  7.11it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "epoch 1 iter 22: train loss 3.36057. lr 5.920992e-06: 100%|██████████| 23/23 [00:05<00:00,  4.58it/s]\n",
            "epoch 2 iter 22: train loss 2.97894. lr 1.184198e-05: 100%|██████████| 23/23 [00:03<00:00,  6.46it/s]\n",
            "epoch 3 iter 22: train loss 2.46046. lr 1.776298e-05: 100%|██████████| 23/23 [00:03<00:00,  6.84it/s]\n",
            "epoch 4 iter 22: train loss 2.19758. lr 2.368397e-05: 100%|██████████| 23/23 [00:03<00:00,  6.83it/s]\n",
            "epoch 5 iter 22: train loss 2.09207. lr 2.960496e-05: 100%|██████████| 23/23 [00:03<00:00,  6.71it/s]\n",
            "epoch 6 iter 22: train loss 1.81964. lr 3.552595e-05: 100%|██████████| 23/23 [00:03<00:00,  6.65it/s]\n",
            "epoch 7 iter 22: train loss 1.84260. lr 4.144694e-05: 100%|██████████| 23/23 [00:03<00:00,  6.88it/s]\n",
            "epoch 8 iter 22: train loss 1.67697. lr 4.736794e-05: 100%|██████████| 23/23 [00:03<00:00,  6.79it/s]\n",
            "epoch 9 iter 22: train loss 1.56110. lr 5.328893e-05: 100%|██████████| 23/23 [00:03<00:00,  6.61it/s]\n",
            "epoch 10 iter 22: train loss 1.48077. lr 5.920992e-05: 100%|██████████| 23/23 [00:03<00:00,  6.70it/s]\n",
            "epoch 11 iter 22: train loss 1.34951. lr 6.513091e-05: 100%|██████████| 23/23 [00:03<00:00,  6.77it/s]\n",
            "epoch 12 iter 22: train loss 1.38787. lr 7.105190e-05: 100%|██████████| 23/23 [00:03<00:00,  6.68it/s]\n",
            "epoch 13 iter 22: train loss 1.23332. lr 7.697290e-05: 100%|██████████| 23/23 [00:03<00:00,  6.52it/s]\n",
            "epoch 14 iter 22: train loss 1.23839. lr 8.289389e-05: 100%|██████████| 23/23 [00:03<00:00,  6.70it/s]\n",
            "epoch 15 iter 22: train loss 1.27954. lr 8.881488e-05: 100%|██████████| 23/23 [00:03<00:00,  6.73it/s]\n",
            "epoch 16 iter 22: train loss 1.22952. lr 9.473587e-05: 100%|██████████| 23/23 [00:03<00:00,  6.56it/s]\n",
            "epoch 17 iter 22: train loss 1.18391. lr 1.006569e-04: 100%|██████████| 23/23 [00:03<00:00,  6.44it/s]\n",
            "epoch 18 iter 22: train loss 1.25722. lr 1.065779e-04: 100%|██████████| 23/23 [00:03<00:00,  6.59it/s]\n",
            "epoch 19 iter 22: train loss 1.12701. lr 1.124988e-04: 100%|██████████| 23/23 [00:03<00:00,  6.73it/s]\n",
            "epoch 20 iter 22: train loss 1.09842. lr 1.184198e-04: 100%|██████████| 23/23 [00:03<00:00,  6.51it/s]\n",
            "epoch 21 iter 22: train loss 1.19021. lr 1.243408e-04: 100%|██████████| 23/23 [00:03<00:00,  6.61it/s]\n",
            "epoch 22 iter 22: train loss 1.16791. lr 1.302618e-04: 100%|██████████| 23/23 [00:03<00:00,  6.57it/s]\n",
            "epoch 23 iter 22: train loss 1.17209. lr 1.361828e-04: 100%|██████████| 23/23 [00:03<00:00,  6.51it/s]\n",
            "epoch 24 iter 22: train loss 1.07397. lr 1.421038e-04: 100%|██████████| 23/23 [00:03<00:00,  6.40it/s]\n",
            "epoch 25 iter 22: train loss 1.09792. lr 1.480248e-04: 100%|██████████| 23/23 [00:03<00:00,  6.56it/s]\n",
            "epoch 26 iter 22: train loss 1.09497. lr 1.539458e-04: 100%|██████████| 23/23 [00:03<00:00,  6.58it/s]\n",
            "epoch 27 iter 22: train loss 1.18649. lr 1.598668e-04: 100%|██████████| 23/23 [00:03<00:00,  6.46it/s]\n",
            "epoch 28 iter 22: train loss 1.09875. lr 1.657878e-04: 100%|██████████| 23/23 [00:03<00:00,  6.37it/s]\n",
            "epoch 29 iter 22: train loss 1.01769. lr 1.717088e-04: 100%|██████████| 23/23 [00:03<00:00,  6.49it/s]\n",
            "epoch 30 iter 22: train loss 0.99760. lr 1.776298e-04: 100%|██████████| 23/23 [00:03<00:00,  6.50it/s]\n",
            "epoch 31 iter 22: train loss 1.05191. lr 1.835508e-04: 100%|██████████| 23/23 [00:03<00:00,  6.42it/s]\n",
            "epoch 32 iter 22: train loss 0.96259. lr 1.894717e-04: 100%|██████████| 23/23 [00:03<00:00,  6.46it/s]\n",
            "epoch 33 iter 22: train loss 0.96699. lr 1.953927e-04: 100%|██████████| 23/23 [00:03<00:00,  6.47it/s]\n",
            "epoch 34 iter 22: train loss 1.02943. lr 2.013137e-04: 100%|██████████| 23/23 [00:03<00:00,  6.43it/s]\n",
            "epoch 35 iter 22: train loss 1.04024. lr 2.072347e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 36 iter 22: train loss 1.00845. lr 2.131557e-04: 100%|██████████| 23/23 [00:03<00:00,  6.41it/s]\n",
            "epoch 37 iter 22: train loss 1.08801. lr 2.190767e-04: 100%|██████████| 23/23 [00:03<00:00,  6.37it/s]\n",
            "epoch 38 iter 22: train loss 1.00185. lr 2.249977e-04: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 39 iter 22: train loss 1.07709. lr 2.309187e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 40 iter 22: train loss 0.92468. lr 2.368397e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 41 iter 22: train loss 1.01060. lr 2.427607e-04: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 42 iter 22: train loss 0.97209. lr 2.486817e-04: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 43 iter 22: train loss 0.92744. lr 2.546027e-04: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 44 iter 22: train loss 0.87481. lr 2.605236e-04: 100%|██████████| 23/23 [00:03<00:00,  5.81it/s]\n",
            "epoch 45 iter 22: train loss 0.85864. lr 2.664446e-04: 100%|██████████| 23/23 [00:03<00:00,  5.83it/s]\n",
            "epoch 46 iter 22: train loss 0.98216. lr 2.723656e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 47 iter 22: train loss 0.90364. lr 2.782866e-04: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 48 iter 22: train loss 0.97018. lr 2.842076e-04: 100%|██████████| 23/23 [00:03<00:00,  5.89it/s]\n",
            "epoch 49 iter 22: train loss 0.83878. lr 2.901286e-04: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 50 iter 22: train loss 0.95492. lr 2.960496e-04: 100%|██████████| 23/23 [00:03<00:00,  6.24it/s]\n",
            "epoch 51 iter 22: train loss 0.88519. lr 3.019706e-04: 100%|██████████| 23/23 [00:03<00:00,  6.29it/s]\n",
            "epoch 52 iter 22: train loss 0.84965. lr 3.078916e-04: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 53 iter 22: train loss 0.80302. lr 3.138126e-04: 100%|██████████| 23/23 [00:03<00:00,  6.26it/s]\n",
            "epoch 54 iter 22: train loss 0.86112. lr 3.197336e-04: 100%|██████████| 23/23 [00:03<00:00,  6.30it/s]\n",
            "epoch 55 iter 22: train loss 0.90497. lr 3.256546e-04: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 56 iter 22: train loss 0.81107. lr 3.315756e-04: 100%|██████████| 23/23 [00:03<00:00,  6.32it/s]\n",
            "epoch 57 iter 22: train loss 0.84191. lr 3.374965e-04: 100%|██████████| 23/23 [00:03<00:00,  6.32it/s]\n",
            "epoch 58 iter 22: train loss 0.78660. lr 3.434175e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 59 iter 22: train loss 0.82624. lr 3.493385e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 60 iter 22: train loss 0.78676. lr 3.552595e-04: 100%|██████████| 23/23 [00:03<00:00,  6.27it/s]\n",
            "epoch 61 iter 22: train loss 0.79677. lr 3.611805e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 62 iter 22: train loss 0.81421. lr 3.671015e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 63 iter 22: train loss 0.82162. lr 3.730225e-04: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 64 iter 22: train loss 0.75899. lr 3.789435e-04: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 65 iter 22: train loss 0.77261. lr 3.848645e-04: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 66 iter 22: train loss 0.81352. lr 3.907855e-04: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 67 iter 22: train loss 0.77641. lr 3.967065e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 68 iter 22: train loss 0.82797. lr 4.026275e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 69 iter 22: train loss 0.73953. lr 4.085484e-04: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 70 iter 22: train loss 0.71964. lr 4.144694e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 71 iter 22: train loss 0.72793. lr 4.203904e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 72 iter 22: train loss 0.76317. lr 4.263114e-04: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 73 iter 22: train loss 0.70122. lr 4.322324e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 74 iter 22: train loss 0.75698. lr 4.381534e-04: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 75 iter 22: train loss 0.65145. lr 4.440744e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 76 iter 22: train loss 0.66588. lr 4.499954e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 77 iter 22: train loss 0.69510. lr 4.559164e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 78 iter 22: train loss 0.71503. lr 4.618374e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 79 iter 22: train loss 0.69263. lr 4.677584e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 80 iter 22: train loss 0.75002. lr 4.736794e-04: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 81 iter 22: train loss 0.68641. lr 4.796004e-04: 100%|██████████| 23/23 [00:03<00:00,  6.27it/s]\n",
            "epoch 82 iter 22: train loss 0.65689. lr 4.855213e-04: 100%|██████████| 23/23 [00:03<00:00,  6.24it/s]\n",
            "epoch 83 iter 22: train loss 0.65637. lr 4.914423e-04: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 84 iter 22: train loss 0.67560. lr 4.973633e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 85 iter 22: train loss 0.70671. lr 5.032843e-04: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 86 iter 22: train loss 0.68576. lr 5.092053e-04: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 87 iter 22: train loss 0.70011. lr 5.151263e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 88 iter 22: train loss 0.68553. lr 5.210473e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 89 iter 22: train loss 0.62124. lr 5.269683e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 90 iter 22: train loss 0.66075. lr 5.328893e-04: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 91 iter 22: train loss 0.62527. lr 5.388103e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 92 iter 22: train loss 0.68745. lr 5.447313e-04: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 93 iter 22: train loss 0.64716. lr 5.506523e-04: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 94 iter 22: train loss 0.64422. lr 5.565732e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 95 iter 22: train loss 0.59700. lr 5.624942e-04: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 96 iter 22: train loss 0.64874. lr 5.684152e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 97 iter 22: train loss 0.62247. lr 5.743362e-04: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 98 iter 22: train loss 0.58980. lr 5.802572e-04: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 99 iter 22: train loss 0.61056. lr 5.861782e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 100 iter 22: train loss 0.64047. lr 5.920992e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 101 iter 22: train loss 0.59102. lr 5.980202e-04: 100%|██████████| 23/23 [00:03<00:00,  6.26it/s]\n",
            "epoch 102 iter 22: train loss 0.60742. lr 6.039412e-04: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 103 iter 22: train loss 0.59413. lr 6.098622e-04: 100%|██████████| 23/23 [00:03<00:00,  5.91it/s]\n",
            "epoch 104 iter 22: train loss 0.64931. lr 6.157832e-04: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 105 iter 22: train loss 0.61114. lr 6.217042e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 106 iter 22: train loss 0.62572. lr 6.276252e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 107 iter 22: train loss 0.59370. lr 6.335461e-04: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 108 iter 22: train loss 0.61788. lr 6.394671e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 109 iter 22: train loss 0.55111. lr 6.453881e-04: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 110 iter 22: train loss 0.62438. lr 6.513091e-04: 100%|██████████| 23/23 [00:03<00:00,  5.97it/s]\n",
            "epoch 111 iter 22: train loss 0.59864. lr 6.572301e-04: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 112 iter 22: train loss 0.61554. lr 6.631511e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 113 iter 22: train loss 0.60710. lr 6.690721e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 114 iter 22: train loss 0.61248. lr 6.749931e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 115 iter 22: train loss 0.58743. lr 6.809141e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 116 iter 22: train loss 0.53562. lr 6.868351e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 117 iter 22: train loss 0.59416. lr 6.927561e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 118 iter 22: train loss 0.57636. lr 6.986771e-04: 100%|██████████| 23/23 [00:03<00:00,  6.24it/s]\n",
            "epoch 119 iter 22: train loss 0.54432. lr 7.045980e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 120 iter 22: train loss 0.58412. lr 7.105190e-04: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 121 iter 22: train loss 0.51730. lr 7.164400e-04: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 122 iter 22: train loss 0.54076. lr 7.223610e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 123 iter 22: train loss 0.56364. lr 7.282820e-04: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 124 iter 22: train loss 0.52962. lr 7.342030e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 125 iter 22: train loss 0.56031. lr 7.401240e-04: 100%|██████████| 23/23 [00:03<00:00,  6.25it/s]\n",
            "epoch 126 iter 22: train loss 0.54810. lr 7.460450e-04: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 127 iter 22: train loss 0.58572. lr 7.519660e-04: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 128 iter 22: train loss 0.54806. lr 7.578870e-04: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 129 iter 22: train loss 0.58058. lr 7.638080e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 130 iter 22: train loss 0.56081. lr 7.697290e-04: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 131 iter 22: train loss 0.57546. lr 7.756500e-04: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 132 iter 22: train loss 0.54352. lr 7.815709e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 133 iter 22: train loss 0.52485. lr 7.874919e-04: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 134 iter 22: train loss 0.52068. lr 7.934129e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 135 iter 22: train loss 0.53750. lr 7.993339e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 136 iter 22: train loss 0.52447. lr 8.052549e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 137 iter 22: train loss 0.53673. lr 8.111759e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 138 iter 22: train loss 0.53746. lr 8.170969e-04: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 139 iter 22: train loss 0.54037. lr 8.230179e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 140 iter 22: train loss 0.54013. lr 8.289389e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 141 iter 22: train loss 0.54603. lr 8.348599e-04: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 142 iter 22: train loss 0.55566. lr 8.407809e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 143 iter 22: train loss 0.51907. lr 8.467019e-04: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 144 iter 22: train loss 0.46434. lr 8.526228e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 145 iter 22: train loss 0.55384. lr 8.585438e-04: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 146 iter 22: train loss 0.54698. lr 8.644648e-04: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 147 iter 22: train loss 0.51537. lr 8.703858e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 148 iter 22: train loss 0.49540. lr 8.763068e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 149 iter 22: train loss 0.50690. lr 8.822278e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 150 iter 22: train loss 0.50946. lr 8.881488e-04: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 151 iter 22: train loss 0.47482. lr 8.940698e-04: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 152 iter 22: train loss 0.49764. lr 8.999908e-04: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 153 iter 22: train loss 0.52050. lr 9.059118e-04: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 154 iter 22: train loss 0.49610. lr 9.118328e-04: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 155 iter 22: train loss 0.54441. lr 9.177538e-04: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 156 iter 22: train loss 0.46083. lr 9.236748e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 157 iter 22: train loss 0.50948. lr 9.295957e-04: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 158 iter 22: train loss 0.47007. lr 9.355167e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 159 iter 22: train loss 0.49095. lr 9.414377e-04: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 160 iter 22: train loss 0.50171. lr 9.473587e-04: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 161 iter 22: train loss 0.47519. lr 9.532797e-04: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 162 iter 22: train loss 0.49906. lr 9.592007e-04: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 163 iter 22: train loss 0.48690. lr 9.651217e-04: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 164 iter 22: train loss 0.45220. lr 9.710427e-04: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 165 iter 22: train loss 0.46293. lr 9.769637e-04: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 166 iter 22: train loss 0.51751. lr 9.828847e-04: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 167 iter 22: train loss 0.46583. lr 9.888057e-04: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 168 iter 22: train loss 0.45244. lr 9.947267e-04: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 169 iter 22: train loss 0.44531. lr 1.000648e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 170 iter 22: train loss 0.50162. lr 1.006569e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 171 iter 22: train loss 0.46281. lr 1.012490e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 172 iter 22: train loss 0.48553. lr 1.018411e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 173 iter 22: train loss 0.48416. lr 1.024332e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 174 iter 22: train loss 0.44160. lr 1.030253e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 175 iter 22: train loss 0.47605. lr 1.036174e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 176 iter 22: train loss 0.44092. lr 1.042095e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 177 iter 22: train loss 0.45706. lr 1.048016e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 178 iter 22: train loss 0.45374. lr 1.053937e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 179 iter 22: train loss 0.44450. lr 1.059858e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 180 iter 22: train loss 0.42022. lr 1.065779e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 181 iter 22: train loss 0.46166. lr 1.071700e-03: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 182 iter 22: train loss 0.46450. lr 1.077621e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 183 iter 22: train loss 0.44800. lr 1.083542e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 184 iter 22: train loss 0.45897. lr 1.089463e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 185 iter 22: train loss 0.45525. lr 1.095384e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 186 iter 22: train loss 0.51491. lr 1.101305e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 187 iter 22: train loss 0.43716. lr 1.107226e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 188 iter 22: train loss 0.45626. lr 1.113146e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 189 iter 22: train loss 0.42630. lr 1.119067e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 190 iter 22: train loss 0.44120. lr 1.124988e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 191 iter 22: train loss 0.45716. lr 1.130909e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 192 iter 22: train loss 0.44337. lr 1.136830e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 193 iter 22: train loss 0.44684. lr 1.142751e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 194 iter 22: train loss 0.43491. lr 1.148672e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 195 iter 22: train loss 0.41438. lr 1.154593e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 196 iter 22: train loss 0.45483. lr 1.160514e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 197 iter 22: train loss 0.43723. lr 1.166435e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 198 iter 22: train loss 0.46702. lr 1.172356e-03: 100%|██████████| 23/23 [00:03<00:00,  6.25it/s]\n",
            "epoch 199 iter 22: train loss 0.41749. lr 1.178277e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 200 iter 22: train loss 0.43241. lr 1.184198e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 201 iter 22: train loss 0.44158. lr 1.190119e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 202 iter 22: train loss 0.45256. lr 1.196040e-03: 100%|██████████| 23/23 [00:03<00:00,  6.23it/s]\n",
            "epoch 203 iter 22: train loss 0.44061. lr 1.201961e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 204 iter 22: train loss 0.41496. lr 1.207882e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 205 iter 22: train loss 0.41992. lr 1.213803e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 206 iter 22: train loss 0.42172. lr 1.219724e-03: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 207 iter 22: train loss 0.45175. lr 1.225645e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 208 iter 22: train loss 0.45964. lr 1.231566e-03: 100%|██████████| 23/23 [00:03<00:00,  6.25it/s]\n",
            "epoch 209 iter 22: train loss 0.42045. lr 1.237487e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 210 iter 22: train loss 0.43381. lr 1.243408e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 211 iter 22: train loss 0.42999. lr 1.249329e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 212 iter 22: train loss 0.43630. lr 1.255250e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 213 iter 22: train loss 0.40819. lr 1.261171e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 214 iter 22: train loss 0.42877. lr 1.267092e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 215 iter 22: train loss 0.40247. lr 1.273013e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 216 iter 22: train loss 0.44341. lr 1.278934e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 217 iter 22: train loss 0.41757. lr 1.284855e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 218 iter 22: train loss 0.38678. lr 1.290776e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 219 iter 22: train loss 0.42615. lr 1.296697e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 220 iter 22: train loss 0.44170. lr 1.302618e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 221 iter 22: train loss 0.44786. lr 1.308539e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 222 iter 22: train loss 0.44315. lr 1.314460e-03: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 223 iter 22: train loss 0.43382. lr 1.320381e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 224 iter 22: train loss 0.41738. lr 1.326302e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 225 iter 22: train loss 0.39991. lr 1.332223e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 226 iter 22: train loss 0.41044. lr 1.338144e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 227 iter 22: train loss 0.44678. lr 1.344065e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 228 iter 22: train loss 0.41362. lr 1.349986e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 229 iter 22: train loss 0.40956. lr 1.355907e-03: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 230 iter 22: train loss 0.43648. lr 1.361828e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 231 iter 22: train loss 0.41297. lr 1.367749e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 232 iter 22: train loss 0.41500. lr 1.373670e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 233 iter 22: train loss 0.37319. lr 1.379591e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 234 iter 22: train loss 0.41367. lr 1.385512e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 235 iter 22: train loss 0.43828. lr 1.391433e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 236 iter 22: train loss 0.41722. lr 1.397354e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 237 iter 22: train loss 0.40101. lr 1.403275e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 238 iter 22: train loss 0.41391. lr 1.409196e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 239 iter 22: train loss 0.40724. lr 1.415117e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 240 iter 22: train loss 0.41896. lr 1.421038e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 241 iter 22: train loss 0.44900. lr 1.426959e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 242 iter 22: train loss 0.42328. lr 1.432880e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 243 iter 22: train loss 0.38974. lr 1.438801e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 244 iter 22: train loss 0.40901. lr 1.444722e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 245 iter 22: train loss 0.42690. lr 1.450643e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 246 iter 22: train loss 0.37273. lr 1.456564e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 247 iter 22: train loss 0.40698. lr 1.462485e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 248 iter 22: train loss 0.40847. lr 1.468406e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 249 iter 22: train loss 0.42913. lr 1.474327e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 250 iter 22: train loss 0.40655. lr 1.480248e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 251 iter 22: train loss 0.37477. lr 1.486169e-03: 100%|██████████| 23/23 [00:03<00:00,  5.97it/s]\n",
            "epoch 252 iter 22: train loss 0.40665. lr 1.492090e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 253 iter 22: train loss 0.37509. lr 1.498011e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 254 iter 22: train loss 0.39929. lr 1.503932e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 255 iter 22: train loss 0.41217. lr 1.509853e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 256 iter 22: train loss 0.41525. lr 1.515774e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 257 iter 22: train loss 0.42074. lr 1.521695e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 258 iter 22: train loss 0.38136. lr 1.527616e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 259 iter 22: train loss 0.38397. lr 1.533537e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 260 iter 22: train loss 0.38431. lr 1.539458e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 261 iter 22: train loss 0.40530. lr 1.545379e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 262 iter 22: train loss 0.42609. lr 1.551300e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 263 iter 22: train loss 0.39679. lr 1.557221e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 264 iter 22: train loss 0.39499. lr 1.563142e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 265 iter 22: train loss 0.39960. lr 1.569063e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 266 iter 22: train loss 0.39494. lr 1.574984e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 267 iter 22: train loss 0.41249. lr 1.580905e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 268 iter 22: train loss 0.40784. lr 1.586826e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 269 iter 22: train loss 0.39859. lr 1.592747e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 270 iter 22: train loss 0.40274. lr 1.598668e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 271 iter 22: train loss 0.41967. lr 1.604589e-03: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 272 iter 22: train loss 0.41971. lr 1.610510e-03: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 273 iter 22: train loss 0.37975. lr 1.616431e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 274 iter 22: train loss 0.36880. lr 1.622352e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 275 iter 22: train loss 0.39872. lr 1.628273e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 276 iter 22: train loss 0.38026. lr 1.634194e-03: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 277 iter 22: train loss 0.41734. lr 1.640115e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 278 iter 22: train loss 0.38264. lr 1.646036e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 279 iter 22: train loss 0.38626. lr 1.651957e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 280 iter 22: train loss 0.40647. lr 1.657878e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 281 iter 22: train loss 0.36523. lr 1.663799e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 282 iter 22: train loss 0.38476. lr 1.669720e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 283 iter 22: train loss 0.34692. lr 1.675641e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 284 iter 22: train loss 0.37764. lr 1.681562e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 285 iter 22: train loss 0.41103. lr 1.687483e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 286 iter 22: train loss 0.38942. lr 1.693404e-03: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 287 iter 22: train loss 0.35159. lr 1.699325e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 288 iter 22: train loss 0.38084. lr 1.705246e-03: 100%|██████████| 23/23 [00:03<00:00,  5.91it/s]\n",
            "epoch 289 iter 22: train loss 0.35280. lr 1.711167e-03: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 290 iter 22: train loss 0.38523. lr 1.717088e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 291 iter 22: train loss 0.37421. lr 1.723009e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 292 iter 22: train loss 0.36124. lr 1.728930e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 293 iter 22: train loss 0.40641. lr 1.734851e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 294 iter 22: train loss 0.36432. lr 1.740772e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 295 iter 22: train loss 0.36706. lr 1.746693e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 296 iter 22: train loss 0.40318. lr 1.752614e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 297 iter 22: train loss 0.38650. lr 1.758535e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 298 iter 22: train loss 0.38892. lr 1.764456e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 299 iter 22: train loss 0.37155. lr 1.770377e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 300 iter 22: train loss 0.38360. lr 1.776298e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 301 iter 22: train loss 0.38110. lr 1.782219e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 302 iter 22: train loss 0.37967. lr 1.788140e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 303 iter 22: train loss 0.36869. lr 1.794061e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 304 iter 22: train loss 0.39864. lr 1.799982e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 305 iter 22: train loss 0.36285. lr 1.805903e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 306 iter 22: train loss 0.38370. lr 1.811824e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 307 iter 22: train loss 0.36997. lr 1.817745e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 308 iter 22: train loss 0.37260. lr 1.823666e-03: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 309 iter 22: train loss 0.35545. lr 1.829587e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 310 iter 22: train loss 0.37077. lr 1.835508e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 311 iter 22: train loss 0.40092. lr 1.841429e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 312 iter 22: train loss 0.37736. lr 1.847350e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 313 iter 22: train loss 0.35928. lr 1.853270e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 314 iter 22: train loss 0.35272. lr 1.859191e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 315 iter 22: train loss 0.37054. lr 1.865112e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 316 iter 22: train loss 0.36477. lr 1.871033e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 317 iter 22: train loss 0.37244. lr 1.876954e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 318 iter 22: train loss 0.37391. lr 1.882875e-03: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 319 iter 22: train loss 0.34906. lr 1.888796e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 320 iter 22: train loss 0.35763. lr 1.894717e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 321 iter 22: train loss 0.37971. lr 1.900638e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 322 iter 22: train loss 0.37156. lr 1.906559e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 323 iter 22: train loss 0.36630. lr 1.912480e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 324 iter 22: train loss 0.37804. lr 1.918401e-03: 100%|██████████| 23/23 [00:03<00:00,  5.88it/s]\n",
            "epoch 325 iter 22: train loss 0.35552. lr 1.924322e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 326 iter 22: train loss 0.36099. lr 1.930243e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 327 iter 22: train loss 0.35529. lr 1.936164e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 328 iter 22: train loss 0.37546. lr 1.942085e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 329 iter 22: train loss 0.35468. lr 1.948006e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 330 iter 22: train loss 0.35713. lr 1.953927e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 331 iter 22: train loss 0.37402. lr 1.959848e-03: 100%|██████████| 23/23 [00:03<00:00,  5.89it/s]\n",
            "epoch 332 iter 22: train loss 0.37978. lr 1.965769e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 333 iter 22: train loss 0.35992. lr 1.971690e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 334 iter 22: train loss 0.35031. lr 1.977611e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 335 iter 22: train loss 0.36013. lr 1.983532e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 336 iter 22: train loss 0.37634. lr 1.989453e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 337 iter 22: train loss 0.35957. lr 1.995374e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 338 iter 22: train loss 0.33132. lr 2.001295e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 339 iter 22: train loss 0.39212. lr 2.007216e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 340 iter 22: train loss 0.37122. lr 2.013137e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 341 iter 22: train loss 0.35304. lr 2.019058e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 342 iter 22: train loss 0.36232. lr 2.024979e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 343 iter 22: train loss 0.35790. lr 2.030900e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 344 iter 22: train loss 0.35180. lr 2.036821e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 345 iter 22: train loss 0.32939. lr 2.042742e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 346 iter 22: train loss 0.37971. lr 2.048663e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 347 iter 22: train loss 0.34624. lr 2.054584e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 348 iter 22: train loss 0.36676. lr 2.060505e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 349 iter 22: train loss 0.34284. lr 2.066426e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 350 iter 22: train loss 0.34416. lr 2.072347e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 351 iter 22: train loss 0.34909. lr 2.078268e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 352 iter 22: train loss 0.40179. lr 2.084189e-03: 100%|██████████| 23/23 [00:03<00:00,  5.89it/s]\n",
            "epoch 353 iter 22: train loss 0.35728. lr 2.090110e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 354 iter 22: train loss 0.37597. lr 2.096031e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 355 iter 22: train loss 0.36537. lr 2.101952e-03: 100%|██████████| 23/23 [00:03<00:00,  5.97it/s]\n",
            "epoch 356 iter 22: train loss 0.34182. lr 2.107873e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 357 iter 22: train loss 0.35534. lr 2.113794e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 358 iter 22: train loss 0.37716. lr 2.119715e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 359 iter 22: train loss 0.33468. lr 2.125636e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 360 iter 22: train loss 0.36126. lr 2.131557e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 361 iter 22: train loss 0.35114. lr 2.137478e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 362 iter 22: train loss 0.35756. lr 2.143399e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 363 iter 22: train loss 0.35581. lr 2.149320e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 364 iter 22: train loss 0.37198. lr 2.155241e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 365 iter 22: train loss 0.35524. lr 2.161162e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 366 iter 22: train loss 0.37696. lr 2.167083e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 367 iter 22: train loss 0.34518. lr 2.173004e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 368 iter 22: train loss 0.34166. lr 2.178925e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 369 iter 22: train loss 0.32778. lr 2.184846e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 370 iter 22: train loss 0.35337. lr 2.190767e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 371 iter 22: train loss 0.32337. lr 2.196688e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 372 iter 22: train loss 0.35550. lr 2.202609e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 373 iter 22: train loss 0.35160. lr 2.208530e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 374 iter 22: train loss 0.36351. lr 2.214451e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 375 iter 22: train loss 0.34424. lr 2.220372e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 376 iter 22: train loss 0.36112. lr 2.226293e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 377 iter 22: train loss 0.34940. lr 2.232214e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 378 iter 22: train loss 0.33995. lr 2.238135e-03: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 379 iter 22: train loss 0.37425. lr 2.244056e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 380 iter 22: train loss 0.35812. lr 2.249977e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 381 iter 22: train loss 0.35045. lr 2.255898e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 382 iter 22: train loss 0.34449. lr 2.261819e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 383 iter 22: train loss 0.34063. lr 2.267740e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 384 iter 22: train loss 0.33037. lr 2.273661e-03: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 385 iter 22: train loss 0.35590. lr 2.279582e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 386 iter 22: train loss 0.35033. lr 2.285503e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 387 iter 22: train loss 0.33657. lr 2.291424e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 388 iter 22: train loss 0.35579. lr 2.297345e-03: 100%|██████████| 23/23 [00:03<00:00,  5.97it/s]\n",
            "epoch 389 iter 22: train loss 0.34321. lr 2.303266e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 390 iter 22: train loss 0.37177. lr 2.309187e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 391 iter 22: train loss 0.35708. lr 2.315108e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 392 iter 22: train loss 0.37561. lr 2.321029e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 393 iter 22: train loss 0.33602. lr 2.326950e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 394 iter 22: train loss 0.35756. lr 2.332871e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 395 iter 22: train loss 0.36197. lr 2.338792e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 396 iter 22: train loss 0.32206. lr 2.344713e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 397 iter 22: train loss 0.35522. lr 2.350634e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 398 iter 22: train loss 0.35233. lr 2.356555e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 399 iter 22: train loss 0.34421. lr 2.362476e-03: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 400 iter 22: train loss 0.34924. lr 2.368397e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 401 iter 22: train loss 0.34198. lr 2.374318e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 402 iter 22: train loss 0.33581. lr 2.380239e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 403 iter 22: train loss 0.33406. lr 2.386160e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 404 iter 22: train loss 0.34695. lr 2.392081e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 405 iter 22: train loss 0.34359. lr 2.398002e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 406 iter 22: train loss 0.32747. lr 2.403923e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 407 iter 22: train loss 0.34471. lr 2.409844e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 408 iter 22: train loss 0.34702. lr 2.415765e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 409 iter 22: train loss 0.33217. lr 2.421686e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 410 iter 22: train loss 0.35938. lr 2.427607e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 411 iter 22: train loss 0.32201. lr 2.433528e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 412 iter 22: train loss 0.36147. lr 2.439449e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 413 iter 22: train loss 0.33535. lr 2.445370e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 414 iter 22: train loss 0.36381. lr 2.451291e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 415 iter 22: train loss 0.34838. lr 2.457212e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 416 iter 22: train loss 0.34136. lr 2.463133e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 417 iter 22: train loss 0.34883. lr 2.469054e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 418 iter 22: train loss 0.34609. lr 2.474975e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 419 iter 22: train loss 0.34220. lr 2.480896e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 420 iter 22: train loss 0.34653. lr 2.486817e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 421 iter 22: train loss 0.34267. lr 2.492738e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 422 iter 22: train loss 0.31214. lr 2.498659e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 423 iter 22: train loss 0.34536. lr 2.504580e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 424 iter 22: train loss 0.36557. lr 2.510501e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 425 iter 22: train loss 0.33265. lr 2.516422e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 426 iter 22: train loss 0.34849. lr 2.522343e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 427 iter 22: train loss 0.33108. lr 2.528264e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 428 iter 22: train loss 0.34359. lr 2.534185e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 429 iter 22: train loss 0.32681. lr 2.540106e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 430 iter 22: train loss 0.34394. lr 2.546027e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 431 iter 22: train loss 0.37162. lr 2.551948e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 432 iter 22: train loss 0.32597. lr 2.557869e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 433 iter 22: train loss 0.35707. lr 2.563790e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 434 iter 22: train loss 0.32807. lr 2.569711e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 435 iter 22: train loss 0.35951. lr 2.575632e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 436 iter 22: train loss 0.35205. lr 2.581553e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 437 iter 22: train loss 0.33143. lr 2.587474e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 438 iter 22: train loss 0.34607. lr 2.593394e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 439 iter 22: train loss 0.32383. lr 2.599315e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 440 iter 22: train loss 0.35018. lr 2.605236e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 441 iter 22: train loss 0.34231. lr 2.611157e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 442 iter 22: train loss 0.33470. lr 2.617078e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 443 iter 22: train loss 0.33170. lr 2.622999e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 444 iter 22: train loss 0.33702. lr 2.628920e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 445 iter 22: train loss 0.33103. lr 2.634841e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 446 iter 22: train loss 0.35906. lr 2.640762e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 447 iter 22: train loss 0.33624. lr 2.646683e-03: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 448 iter 22: train loss 0.31947. lr 2.652604e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 449 iter 22: train loss 0.34210. lr 2.658525e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 450 iter 22: train loss 0.33185. lr 2.664446e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 451 iter 22: train loss 0.33107. lr 2.670367e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 452 iter 22: train loss 0.31210. lr 2.676288e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 453 iter 22: train loss 0.32651. lr 2.682209e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 454 iter 22: train loss 0.33726. lr 2.688130e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 455 iter 22: train loss 0.31006. lr 2.694051e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 456 iter 22: train loss 0.35672. lr 2.699972e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 457 iter 22: train loss 0.36571. lr 2.705893e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 458 iter 22: train loss 0.34948. lr 2.711814e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 459 iter 22: train loss 0.33719. lr 2.717735e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 460 iter 22: train loss 0.35047. lr 2.723656e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 461 iter 22: train loss 0.31029. lr 2.729577e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 462 iter 22: train loss 0.31572. lr 2.735498e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 463 iter 22: train loss 0.31510. lr 2.741419e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 464 iter 22: train loss 0.33547. lr 2.747340e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 465 iter 22: train loss 0.33823. lr 2.753261e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 466 iter 22: train loss 0.31219. lr 2.759182e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 467 iter 22: train loss 0.34531. lr 2.765103e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 468 iter 22: train loss 0.32515. lr 2.771024e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 469 iter 22: train loss 0.33235. lr 2.776945e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 470 iter 22: train loss 0.32750. lr 2.782866e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 471 iter 22: train loss 0.32625. lr 2.788787e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 472 iter 22: train loss 0.34690. lr 2.794708e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 473 iter 22: train loss 0.34057. lr 2.800629e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 474 iter 22: train loss 0.32546. lr 2.806550e-03: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 475 iter 22: train loss 0.34311. lr 2.812471e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 476 iter 22: train loss 0.32382. lr 2.818392e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 477 iter 22: train loss 0.33480. lr 2.824313e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 478 iter 22: train loss 0.33825. lr 2.830234e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 479 iter 22: train loss 0.33768. lr 2.836155e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 480 iter 22: train loss 0.31924. lr 2.842076e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 481 iter 22: train loss 0.34169. lr 2.847997e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 482 iter 22: train loss 0.33045. lr 2.853918e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 483 iter 22: train loss 0.33586. lr 2.859839e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 484 iter 22: train loss 0.33713. lr 2.865760e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 485 iter 22: train loss 0.33076. lr 2.871681e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 486 iter 22: train loss 0.31964. lr 2.877602e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 487 iter 22: train loss 0.33203. lr 2.883523e-03: 100%|██████████| 23/23 [00:03<00:00,  5.97it/s]\n",
            "epoch 488 iter 22: train loss 0.34911. lr 2.889444e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 489 iter 22: train loss 0.31764. lr 2.895365e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 490 iter 22: train loss 0.33859. lr 2.901286e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 491 iter 22: train loss 0.33055. lr 2.907207e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 492 iter 22: train loss 0.33022. lr 2.913128e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 493 iter 22: train loss 0.35299. lr 2.919049e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 494 iter 22: train loss 0.32923. lr 2.924970e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 495 iter 22: train loss 0.34701. lr 2.930891e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 496 iter 22: train loss 0.33208. lr 2.936812e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 497 iter 22: train loss 0.34801. lr 2.942733e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 498 iter 22: train loss 0.33406. lr 2.948654e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 499 iter 22: train loss 0.32379. lr 2.954575e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 500 iter 22: train loss 0.30403. lr 2.960496e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 501 iter 22: train loss 0.34423. lr 2.966417e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 502 iter 22: train loss 0.33674. lr 2.972338e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 503 iter 22: train loss 0.32954. lr 2.978259e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 504 iter 22: train loss 0.35076. lr 2.984180e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 505 iter 22: train loss 0.35472. lr 2.990101e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 506 iter 22: train loss 0.33090. lr 2.996022e-03: 100%|██████████| 23/23 [00:03<00:00,  6.20it/s]\n",
            "epoch 507 iter 22: train loss 0.34416. lr 3.001943e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 508 iter 22: train loss 0.32647. lr 3.007864e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 509 iter 22: train loss 0.34504. lr 3.013785e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 510 iter 22: train loss 0.34675. lr 3.019706e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 511 iter 22: train loss 0.34188. lr 3.025627e-03: 100%|██████████| 23/23 [00:03<00:00,  5.88it/s]\n",
            "epoch 512 iter 22: train loss 0.35235. lr 3.031548e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 513 iter 22: train loss 0.34553. lr 3.037469e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 514 iter 22: train loss 0.35161. lr 3.043390e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 515 iter 22: train loss 0.32437. lr 3.049311e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 516 iter 22: train loss 0.30519. lr 3.055232e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 517 iter 22: train loss 0.29857. lr 3.061153e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 518 iter 22: train loss 0.30725. lr 3.067074e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 519 iter 22: train loss 0.32544. lr 3.072995e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 520 iter 22: train loss 0.32460. lr 3.078916e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 521 iter 22: train loss 0.31294. lr 3.084837e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 522 iter 22: train loss 0.32273. lr 3.090758e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 523 iter 22: train loss 0.34002. lr 3.096679e-03: 100%|██████████| 23/23 [00:03<00:00,  6.21it/s]\n",
            "epoch 524 iter 22: train loss 0.32980. lr 3.102600e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 525 iter 22: train loss 0.34121. lr 3.108521e-03: 100%|██████████| 23/23 [00:03<00:00,  5.90it/s]\n",
            "epoch 526 iter 22: train loss 0.37821. lr 3.114442e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 527 iter 22: train loss 0.34950. lr 3.120363e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 528 iter 22: train loss 0.32228. lr 3.126284e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 529 iter 22: train loss 0.32930. lr 3.132205e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 530 iter 22: train loss 0.33031. lr 3.138126e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 531 iter 22: train loss 0.31146. lr 3.144047e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 532 iter 22: train loss 0.34027. lr 3.149968e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 533 iter 22: train loss 0.30839. lr 3.155889e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 534 iter 22: train loss 0.31458. lr 3.161810e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 535 iter 22: train loss 0.33751. lr 3.167731e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 536 iter 22: train loss 0.35815. lr 3.173652e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 537 iter 22: train loss 0.34211. lr 3.179573e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 538 iter 22: train loss 0.31602. lr 3.185494e-03: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 539 iter 22: train loss 0.32887. lr 3.191415e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 540 iter 22: train loss 0.32079. lr 3.197336e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 541 iter 22: train loss 0.33216. lr 3.203257e-03: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 542 iter 22: train loss 0.30442. lr 3.209178e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 543 iter 22: train loss 0.31326. lr 3.215099e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 544 iter 22: train loss 0.34969. lr 3.221020e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 545 iter 22: train loss 0.31175. lr 3.226941e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 546 iter 22: train loss 0.32075. lr 3.232862e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 547 iter 22: train loss 0.33212. lr 3.238783e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 548 iter 22: train loss 0.31539. lr 3.244704e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 549 iter 22: train loss 0.35216. lr 3.250625e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 550 iter 22: train loss 0.33158. lr 3.256546e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 551 iter 22: train loss 0.32131. lr 3.262467e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 552 iter 22: train loss 0.33279. lr 3.268388e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 553 iter 22: train loss 0.30925. lr 3.274309e-03: 100%|██████████| 23/23 [00:03<00:00,  6.17it/s]\n",
            "epoch 554 iter 22: train loss 0.32625. lr 3.280230e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 555 iter 22: train loss 0.32857. lr 3.286151e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 556 iter 22: train loss 0.32548. lr 3.292072e-03: 100%|██████████| 23/23 [00:03<00:00,  6.18it/s]\n",
            "epoch 557 iter 22: train loss 0.31879. lr 3.297993e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 558 iter 22: train loss 0.33378. lr 3.303914e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 559 iter 22: train loss 0.31781. lr 3.309835e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 560 iter 22: train loss 0.32084. lr 3.315756e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 561 iter 22: train loss 0.33793. lr 3.321677e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 562 iter 22: train loss 0.34389. lr 3.327598e-03: 100%|██████████| 23/23 [00:03<00:00,  5.96it/s]\n",
            "epoch 563 iter 22: train loss 0.33818. lr 3.333518e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 564 iter 22: train loss 0.32715. lr 3.339439e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 565 iter 22: train loss 0.30746. lr 3.345360e-03: 100%|██████████| 23/23 [00:03<00:00,  5.90it/s]\n",
            "epoch 566 iter 22: train loss 0.34142. lr 3.351281e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 567 iter 22: train loss 0.31517. lr 3.357202e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 568 iter 22: train loss 0.31159. lr 3.363123e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 569 iter 22: train loss 0.31224. lr 3.369044e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 570 iter 22: train loss 0.33075. lr 3.374965e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 571 iter 22: train loss 0.33513. lr 3.380886e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 572 iter 22: train loss 0.31742. lr 3.386807e-03: 100%|██████████| 23/23 [00:03<00:00,  5.88it/s]\n",
            "epoch 573 iter 22: train loss 0.32850. lr 3.392728e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 574 iter 22: train loss 0.32507. lr 3.398649e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 575 iter 22: train loss 0.32167. lr 3.404570e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 576 iter 22: train loss 0.33235. lr 3.410491e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 577 iter 22: train loss 0.32340. lr 3.416412e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 578 iter 22: train loss 0.30521. lr 3.422333e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 579 iter 22: train loss 0.31856. lr 3.428254e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 580 iter 22: train loss 0.32568. lr 3.434175e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 581 iter 22: train loss 0.33804. lr 3.440096e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 582 iter 22: train loss 0.32197. lr 3.446017e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 583 iter 22: train loss 0.32490. lr 3.451938e-03: 100%|██████████| 23/23 [00:03<00:00,  5.93it/s]\n",
            "epoch 584 iter 22: train loss 0.33843. lr 3.457859e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 585 iter 22: train loss 0.32556. lr 3.463780e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 586 iter 22: train loss 0.31234. lr 3.469701e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 587 iter 22: train loss 0.34169. lr 3.475622e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 588 iter 22: train loss 0.33336. lr 3.481543e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 589 iter 22: train loss 0.31850. lr 3.487464e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 590 iter 22: train loss 0.29840. lr 3.493385e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 591 iter 22: train loss 0.34113. lr 3.499306e-03: 100%|██████████| 23/23 [00:03<00:00,  6.14it/s]\n",
            "epoch 592 iter 22: train loss 0.29299. lr 3.505227e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 593 iter 22: train loss 0.32576. lr 3.511148e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 594 iter 22: train loss 0.32029. lr 3.517069e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 595 iter 22: train loss 0.30471. lr 3.522990e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 596 iter 22: train loss 0.33842. lr 3.528911e-03: 100%|██████████| 23/23 [00:03<00:00,  5.95it/s]\n",
            "epoch 597 iter 22: train loss 0.32167. lr 3.534832e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 598 iter 22: train loss 0.32042. lr 3.540753e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 599 iter 22: train loss 0.32993. lr 3.546674e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 600 iter 22: train loss 0.29662. lr 3.552595e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 601 iter 22: train loss 0.31299. lr 3.558516e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 602 iter 22: train loss 0.32189. lr 3.564437e-03: 100%|██████████| 23/23 [00:03<00:00,  6.05it/s]\n",
            "epoch 603 iter 22: train loss 0.33096. lr 3.570358e-03: 100%|██████████| 23/23 [00:03<00:00,  5.90it/s]\n",
            "epoch 604 iter 22: train loss 0.32990. lr 3.576279e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 605 iter 22: train loss 0.31732. lr 3.582200e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 606 iter 22: train loss 0.33955. lr 3.588121e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 607 iter 22: train loss 0.31269. lr 3.594042e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 608 iter 22: train loss 0.33134. lr 3.599963e-03: 100%|██████████| 23/23 [00:03<00:00,  6.13it/s]\n",
            "epoch 609 iter 22: train loss 0.33381. lr 3.605884e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 610 iter 22: train loss 0.32672. lr 3.611805e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 611 iter 22: train loss 0.31702. lr 3.617726e-03: 100%|██████████| 23/23 [00:03<00:00,  6.19it/s]\n",
            "epoch 612 iter 22: train loss 0.33001. lr 3.623647e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 613 iter 22: train loss 0.31815. lr 3.629568e-03: 100%|██████████| 23/23 [00:03<00:00,  6.04it/s]\n",
            "epoch 614 iter 22: train loss 0.30942. lr 3.635489e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 615 iter 22: train loss 0.34659. lr 3.641410e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 616 iter 22: train loss 0.32099. lr 3.647331e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 617 iter 22: train loss 0.31605. lr 3.653252e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 618 iter 22: train loss 0.31065. lr 3.659173e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 619 iter 22: train loss 0.31683. lr 3.665094e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 620 iter 22: train loss 0.32105. lr 3.671015e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 621 iter 22: train loss 0.34116. lr 3.676936e-03: 100%|██████████| 23/23 [00:03<00:00,  6.22it/s]\n",
            "epoch 622 iter 22: train loss 0.33379. lr 3.682857e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 623 iter 22: train loss 0.33659. lr 3.688778e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 624 iter 22: train loss 0.35190. lr 3.694699e-03: 100%|██████████| 23/23 [00:03<00:00,  6.06it/s]\n",
            "epoch 625 iter 22: train loss 0.33121. lr 3.700620e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 626 iter 22: train loss 0.33080. lr 3.706541e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 627 iter 22: train loss 0.29600. lr 3.712462e-03: 100%|██████████| 23/23 [00:03<00:00,  6.02it/s]\n",
            "epoch 628 iter 22: train loss 0.34018. lr 3.718383e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 629 iter 22: train loss 0.31969. lr 3.724304e-03: 100%|██████████| 23/23 [00:03<00:00,  6.16it/s]\n",
            "epoch 630 iter 22: train loss 0.31709. lr 3.730225e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 631 iter 22: train loss 0.34728. lr 3.736146e-03: 100%|██████████| 23/23 [00:03<00:00,  6.08it/s]\n",
            "epoch 632 iter 22: train loss 0.35016. lr 3.742067e-03: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]\n",
            "epoch 633 iter 22: train loss 0.32745. lr 3.747988e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 634 iter 22: train loss 0.31837. lr 3.753909e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n",
            "epoch 635 iter 22: train loss 0.31504. lr 3.759830e-03: 100%|██████████| 23/23 [00:03<00:00,  6.15it/s]\n",
            "epoch 636 iter 22: train loss 0.34268. lr 3.765751e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 637 iter 22: train loss 0.32284. lr 3.771672e-03: 100%|██████████| 23/23 [00:03<00:00,  5.98it/s]\n",
            "epoch 638 iter 22: train loss 0.32298. lr 3.777593e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 639 iter 22: train loss 0.31981. lr 3.783514e-03: 100%|██████████| 23/23 [00:03<00:00,  6.11it/s]\n",
            "epoch 640 iter 22: train loss 0.35773. lr 3.789435e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 641 iter 22: train loss 0.32417. lr 3.795356e-03: 100%|██████████| 23/23 [00:03<00:00,  5.92it/s]\n",
            "epoch 642 iter 22: train loss 0.31923. lr 3.801277e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 643 iter 22: train loss 0.32345. lr 3.807198e-03: 100%|██████████| 23/23 [00:03<00:00,  6.07it/s]\n",
            "epoch 644 iter 22: train loss 0.32810. lr 3.813119e-03: 100%|██████████| 23/23 [00:03<00:00,  6.01it/s]\n",
            "epoch 645 iter 22: train loss 0.33358. lr 3.819040e-03: 100%|██████████| 23/23 [00:03<00:00,  6.12it/s]\n",
            "epoch 646 iter 22: train loss 0.31695. lr 3.824961e-03: 100%|██████████| 23/23 [00:03<00:00,  6.10it/s]\n",
            "epoch 647 iter 22: train loss 0.32984. lr 3.830882e-03: 100%|██████████| 23/23 [00:03<00:00,  5.94it/s]\n",
            "epoch 648 iter 22: train loss 0.35273. lr 3.836803e-03: 100%|██████████| 23/23 [00:03<00:00,  5.99it/s]\n",
            "epoch 649 iter 22: train loss 0.33055. lr 3.842724e-03: 100%|██████████| 23/23 [00:03<00:00,  6.09it/s]\n",
            "epoch 650 iter 22: train loss 0.30411. lr 3.848645e-03: 100%|██████████| 23/23 [00:03<00:00,  6.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune the model\n",
        "finetune('finetune.params', reading_params_path=\"pretrain.params\")"
      ],
      "metadata": {
        "id": "pERFkmFMD3E-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e405aecf-29da-4057-81d7-eb3443dd6701"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 7: train loss 0.11269. lr 5.999844e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 2 iter 7: train loss 0.06542. lr 5.999351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 3 iter 7: train loss 0.04771. lr 5.998520e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "epoch 4 iter 7: train loss 0.04401. lr 5.997351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 5 iter 7: train loss 0.03792. lr 5.995844e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 6 iter 7: train loss 0.03245. lr 5.993999e-04: 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\n",
            "epoch 7 iter 7: train loss 0.03073. lr 5.991818e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 8 iter 7: train loss 0.02710. lr 5.989299e-04: 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]\n",
            "epoch 9 iter 7: train loss 0.02331. lr 5.986444e-04: 100%|██████████| 8/8 [00:02<00:00,  2.85it/s]\n",
            "epoch 10 iter 7: train loss 0.02092. lr 5.983252e-04: 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "epoch 11 iter 7: train loss 0.01910. lr 5.979723e-04: 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "epoch 12 iter 7: train loss 0.01554. lr 5.975860e-04: 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "epoch 13 iter 7: train loss 0.01274. lr 5.971660e-04: 100%|██████████| 8/8 [00:02<00:00,  2.92it/s]\n",
            "epoch 14 iter 7: train loss 0.01179. lr 5.967127e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 15 iter 7: train loss 0.00833. lr 5.962258e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 16 iter 7: train loss 0.00767. lr 5.957056e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 17 iter 7: train loss 0.00895. lr 5.951521e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 18 iter 7: train loss 0.00579. lr 5.945654e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 19 iter 7: train loss 0.00434. lr 5.939454e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 20 iter 7: train loss 0.00496. lr 5.932923e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 21 iter 7: train loss 0.00384. lr 5.926062e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 22 iter 7: train loss 0.00427. lr 5.918871e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 23 iter 7: train loss 0.00382. lr 5.911352e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 24 iter 7: train loss 0.00330. lr 5.903504e-04: 100%|██████████| 8/8 [00:02<00:00,  3.20it/s]\n",
            "epoch 25 iter 7: train loss 0.00307. lr 5.895329e-04: 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]\n",
            "epoch 26 iter 7: train loss 0.00239. lr 5.886828e-04: 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "epoch 27 iter 7: train loss 0.00281. lr 5.878002e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 28 iter 7: train loss 0.00223. lr 5.868851e-04: 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]\n",
            "epoch 29 iter 7: train loss 0.00161. lr 5.859378e-04: 100%|██████████| 8/8 [00:02<00:00,  3.19it/s]\n",
            "epoch 30 iter 7: train loss 0.00165. lr 5.849582e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 31 iter 7: train loss 0.00184. lr 5.839465e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 32 iter 7: train loss 0.00223. lr 5.829028e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 33 iter 7: train loss 0.00167. lr 5.818272e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 34 iter 7: train loss 0.00134. lr 5.807199e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 35 iter 7: train loss 0.00158. lr 5.795810e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 36 iter 7: train loss 0.00145. lr 5.784106e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 37 iter 7: train loss 0.00176. lr 5.772087e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 38 iter 7: train loss 0.00197. lr 5.759757e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 39 iter 7: train loss 0.00108. lr 5.747116e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 40 iter 7: train loss 0.00155. lr 5.734165e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 41 iter 7: train loss 0.00114. lr 5.720906e-04: 100%|██████████| 8/8 [00:02<00:00,  2.92it/s]\n",
            "epoch 42 iter 7: train loss 0.00087. lr 5.707341e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 43 iter 7: train loss 0.00111. lr 5.693470e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 44 iter 7: train loss 0.00185. lr 5.679296e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 45 iter 7: train loss 0.00084. lr 5.664820e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 46 iter 7: train loss 0.00080. lr 5.650044e-04: 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]\n",
            "epoch 47 iter 7: train loss 0.00136. lr 5.634970e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 48 iter 7: train loss 0.00106. lr 5.619598e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 49 iter 7: train loss 0.00139. lr 5.603932e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 50 iter 7: train loss 0.00055. lr 5.587972e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 51 iter 7: train loss 0.00073. lr 5.571720e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 52 iter 7: train loss 0.00055. lr 5.555179e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 53 iter 7: train loss 0.00107. lr 5.538350e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 54 iter 7: train loss 0.00095. lr 5.521235e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 55 iter 7: train loss 0.00070. lr 5.503835e-04: 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]\n",
            "epoch 56 iter 7: train loss 0.00070. lr 5.486154e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 57 iter 7: train loss 0.00055. lr 5.468193e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 58 iter 7: train loss 0.00077. lr 5.449953e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 59 iter 7: train loss 0.00085. lr 5.431438e-04: 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]\n",
            "epoch 60 iter 7: train loss 0.00090. lr 5.412648e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 61 iter 7: train loss 0.00060. lr 5.393587e-04: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n",
            "epoch 62 iter 7: train loss 0.00082. lr 5.374256e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 63 iter 7: train loss 0.00034. lr 5.354657e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 64 iter 7: train loss 0.00042. lr 5.334794e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 65 iter 7: train loss 0.00059. lr 5.314667e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 66 iter 7: train loss 0.00060. lr 5.294279e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 67 iter 7: train loss 0.00034. lr 5.273633e-04: 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\n",
            "epoch 68 iter 7: train loss 0.00062. lr 5.252731e-04: 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\n",
            "epoch 69 iter 7: train loss 0.00113. lr 5.231575e-04: 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\n",
            "epoch 70 iter 7: train loss 0.00044. lr 5.210167e-04: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "epoch 71 iter 7: train loss 0.00067. lr 5.188511e-04: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "epoch 72 iter 7: train loss 0.00044. lr 5.166608e-04: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "epoch 73 iter 7: train loss 0.00062. lr 5.144461e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 74 iter 7: train loss 0.00035. lr 5.122072e-04: 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
            "epoch 75 iter 7: train loss 0.00063. lr 5.099444e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the dev set\n",
        "evaluate(outputs_path=\"pretrain.dev.predictions\", reading_params_path=\"finetune.params\", eval_corpus_path=\"birth_dev.tsv\")"
      ],
      "metadata": {
        "id": "EXz-EEq8D6Hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396bf66c-060e-4af6-9291-6e37ee9e6570"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 418352 characters, 256 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [00:55,  9.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 106.0 out of 500.0: 21.2%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We expect the dev accuracy will be at least 10%."
      ],
      "metadata": {
        "id": "7e8ZlzOfYJ7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "from requests import get\n",
        "from urllib.parse import unquote\n",
        "\n",
        "drive.mount('/mnt/')\n",
        "filename = 'Assignment5.ipynb'\n",
        "filepath = f'/mnt/My Drive/Colab Notebooks/{filename}'\n",
        "output_file = '/content/Assignment5.html'\n",
        "\n",
        "!jupyter nbconvert '{filepath}' --output '{output_file}' --to 'html'\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "og_GHQIESAJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ce436619-e8c8-4cd6-e29d-ea7464038776"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mnt/\n",
            "[NbConvertApp] Converting notebook /mnt/My Drive/Colab Notebooks/Assignment5.ipynb to html\n",
            "[NbConvertApp] Writing 812816 bytes to /content/Assignment5.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b6b8ab03-d481-46df-845d-86f8823fa887\", \"Assignment5.html\", 833851)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}